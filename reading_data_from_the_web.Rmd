---
title: "Reading Data From The Web"
output: github_document
date: "2022-10-13"
---

```{r setup, include = FALSE}
library(tidyverse)
library(rvest)
library(httr)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

## Extracting Tables

### NSDUH Dataset

Using `read_html` in the rvest package, we can read in the html from a webpage.

```{r}
url = "http://samhda.s3-us-gov-west-1.amazonaws.com/s3fs-public/field-uploads/2k15StateFiles/NSDUHsaeShortTermCHG2015.htm"
drug_use_html = read_html(url)

drug_use_html
```

```{r}
drug_use_html %>% 
  html_table() %>% 
  first() %>% 
  slice(-1)
```

`html_table` pulls out all tables from the webpage. It's not giving a dataframe, but a list (a special structure for storing information), and so we can use `first` to pull out the first item from the list and output it as a table.

The “note” at the bottom of the table appears in every column in the first row. We need to remove that using `slice(-1)`.

## CSS Selectors 

### Star Wars movies

Suppose we’d like to scrape the data about the Star Wars Movies from the IMDB page. The first step is the same as before – we need to get the HTML.

```{r}
swm_html = 
  read_html("https://www.imdb.com/list/ls070150896/")
```

The information isn’t stored in a handy table, so we’re going to isolate the CSS selector for elements we care about. A bit of clicking around on Selector Gadget gets me something like below. 

```{r}
sw_titles = swm_html %>% 
  html_elements(".lister-item-header a") %>%   # We get this CSS element from Selector Gadget!
  html_text()

sw_runtime = swm_html %>% 
  html_elements(".runtime") %>% 
  html_text()

sw_gross = swm_html %>% 
  html_elements(".text-muted .ghost~ .text-muted+ span") %>% 
  html_text()

sw_df = 
  tibble(
    title = sw_titles,
    runtime = sw_runtime,
    money = sw_gross
  )

knitr::kable(sw_df)
```

*Learning Assessment*: This page contains the 10 most recent reviews of the movie “Napoleon Dynamite”. Use a process similar to the one above to extract the titles of the reviews.

```{r}
url = "https://www.amazon.com/product-reviews/B00005JNBQ/ref=cm_cr_arp_d_viewopt_rvwer?ie=UTF8&reviewerType=avp_only_reviews&sortBy=recent&pageNumber=1"
# We can change the page number - once we get to pages 3, things start to break

dynamite_html = read_html(url)

review_titles = 
  dynamite_html %>%
  html_elements(".a-text-bold span") %>%
  html_text()

review_stars = 
  dynamite_html %>%
  html_elements("#cm_cr-review_list .review-rating") %>%
  html_text()

review_text = 
  dynamite_html %>%
  html_elements(".review-text-content span") %>%
  html_text()

reviews = tibble(
  title = review_titles,
  stars = review_stars,
  text = review_text
)

reviews
```

## Using an API

New York City has a great open data resource, and we’ll use that for our API examples. Although most (all?) of these datasets can be accessed by clicking through a website, we’ll access them directly using the API to improve reproducibility and make it easier to update results to reflect new data.

```{r}
water_df = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.csv") %>% 
  content("parsed")
```

We can also import this dataset as a JSON file. This takes a bit more work (and this is, really, a pretty easy case), but it’s still doable. The structure of json object notation is a bit different and accomodates more complicated data structures.

```{r}
nyc_water = 
  GET("https://data.cityofnewyork.us/resource/ia2d-e54m.json") %>% 
  content("text") %>%
  jsonlite::fromJSON() %>%
  as_tibble()
```

### BRFSS Data

Data.gov also has a lot of data available using their API; often this is available as CSV or JSON as well. For example, we might be interested in data coming from BRFSS. This is importable via the API as a CSV (JSON, in this example, is more complicated).

```{r}
brfss_df = 
  GET("https://chronicdata.cdc.gov/resource/acme-vg9e.csv", 
      query = list("$limit" = 5000)) %>% 
  content("parsed")

brfss_df
```

By default, the CDC API limits data to the first 1000 rows. Here I’ve increased that by changing an element of the API query – I looked around the website describing the API to find the name of the argument, and then used the appropriate syntax for GET. To get the full data, I could increase this so that I get all the data at once or I could try iterating over chunks of a few thousand rows.


### POKEMON!

Both of the previous examples are, actually, pretty easy – we accessed data that is essentially a data table, and we had a very straightforward API (although updating queries isn’t obvious at first).

To get a sense of how this becomes complicated, let’s look at the Pokemon API (which is also pretty nice).

```{r}
poke = 
  GET("http://pokeapi.co/api/v2/pokemon/1") %>% #Getting pokemon #1
  content()

poke$name
poke[["stats"]]

```

To build a Pokemon dataset for analysis, you’d need to distill the data returned from the API into a useful format; iterate across all pokemon; and combine the results.


